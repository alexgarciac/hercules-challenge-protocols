{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief notebook that includes the automated download of the protocols that will be used for the *Protocols* track of Hercules Challenge. If those protocols have already been downloaded and placed in the _data\\/protocols_ folder, you can skip this and go directly to notebook _2. Data Exploration_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the download of every protocol we have a list of protocol urls that will be fetched. This list has been obtained from the official Hercules challenge documentation, and has been written to the file _protocol\\_urls.txt_ located in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting logger\n"
     ]
    }
   ],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the protocols URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we are going to read every protocol url from the file described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTOCOL_URLS_FILE = os.path.join(DATA_DIR, 'protocol_urls.txt')\n",
    "\n",
    "def get_protocols_urls(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        urls = [url.rstrip() for url in f]\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = get_protocols_urls(PROTOCOL_URLS_FILE)\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 100 protocols that will be used for this track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://bio-protocol.org/e16'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have every protocol url loaded, we are going to define a simple class that will be in charge of fetching the data from the [bio-protocol](https://bio-protocol.org/) website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://bio-protocol.org/\"\n",
    "\n",
    "class BioProtocolScrapper():\n",
    "    def __init__(self, output_dir, throttle_time=.5,\n",
    "                 username=None, password=None):\n",
    "        self.output_dir = output_dir\n",
    "        self.throttle_time = throttle_time\n",
    "        if username and password:\n",
    "            self.login(username, password)\n",
    "    \n",
    "    def fetch_urls(self, url_list):\n",
    "        pbar = tqdm(url_list)\n",
    "        for url in pbar:\n",
    "            pbar.set_description(\"Processing %s\" % url)\n",
    "            self.fetch_url(url)\n",
    "            time.sleep(self.throttle_time)\n",
    "    \n",
    "    def fetch_url(self, url):\n",
    "        filename = url.split('/')[-1] if '/' in url else url\n",
    "        filename += \".html\"\n",
    "        response = requests.get(url)\n",
    "        with open(os.path.join(self.output_dir, filename), 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "    \n",
    "    def _login(self, user, password):\n",
    "        payload = {'txtEmail': user, 'txtPassword': password}\n",
    "        url = f'{BASE_URL}/ifrlogin.aspx/?sign=in&p=4'\n",
    "        requests.post(url, data=payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be making use of this class to fetch every url and save the resulting _html_ to the _data/protocols_ folder. In the following notebook we will be loading and parsing this data to make our initial exploration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing https://bio-protocol.org/e3436: 100%|██████████| 100/100 [03:59<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "scrapper = BioProtocolScrapper(PROTOCOLS_DIR)\n",
    "scrapper.fetch_urls(urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
