{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Text Summarization\n",
    "In this notebook we will be working on the creation of models to obtain a summary of the protocols from this track. We will be exploring two different types of summarization approaches:\n",
    "* Extractive summarization: This approach relies on detecting the important sections of the input text and using those sections to generate a summary which is a subset of the original text. No new text generated with this technique.\n",
    "* Abstractive summarization: This approach reproduces important information from the input text but generates new complementary text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "As always, we will be importing common constants of every notebook from the track, and then we will load the protocols dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting logger\n"
     ]
    }
   ],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DF_FILE_PATH = os.path.join(NOTEBOOK_2_RESULTS_DIR, 'protocols_dataframe.pkl')\n",
    "\n",
    "df = pd.read_pickle(DF_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains a column with the full text of the protocol without its abstract. Since the idea is to generate a summary of the protocol, and the abstract is in itself an abstractive summary, we will try to infer a good summary without relying on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scratch Wound Healing Assay. Grow cells in DMEM supplemented with 10% FBS. Seed cells into 24-well tissue culture plate at a density that after 24 h of growth, they should reach ~70-80% confluence as a monolayer. Do not change the medium. Gently and slowly scratch the monolayer with a new 1 ml pipette tip across the center of the well. While scratching across the surface of the well, the long-axial of the tip should always be perpendicular to the bottom of the well. The resulting gap distance th'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols_no_abstract = df['full_text_no_abstract_cleaned'].values\n",
    "protocols_no_abstract[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive summary\n",
    "For the extractive summary approach we will be relying on the term frequency of each term in a sentence to select the most relevant sentences that will then be added to the final summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "\n",
    "\n",
    "nlp = en_core_sci_lg.load()\n",
    "\n",
    "def extract_summary(text, max_sentences):\n",
    "    keyword = []\n",
    "    pos_tag = ['ADJ', 'NOUN', 'PROPN', 'VERB']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "        \n",
    "    sent_score={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_score.keys():\n",
    "                    sent_score[sent] += freq_word[word.text]\n",
    "                else:\n",
    "                    sent_score[sent] = freq_word[word.text]\n",
    "    \n",
    "    summary = []\n",
    "    sorted_x = sorted(sent_score.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    num_sentences = 0\n",
    "    for i in range(len(sorted_x)):\n",
    "        summary.append(str(sorted_x[i][0]).capitalize())\n",
    "        num_sentences += 1\n",
    "        if num_sentences >= max_sentences:\n",
    "            break\n",
    "            \n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grow cells for additional 48 h (or the time required if different cells are used). Wash the cells twice with 1x pbs, then fix the cells with 3.7% paraformaldehye for 30 min. Seed cells into 24-well tissue culture plate at a density that after 24 h of growth, they should reach ~70-80% confluence as a monolayer. After scratching, gently wash the well twice with medium to remove the detached cells.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sentences = 4\n",
    "\n",
    "extractive_summaries = [extract_summary(t, num_sentences)\n",
    "                        for t in protocols_no_abstract]\n",
    "extractive_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the protocols contain only the materials and procedure (list of steps of the protocol), an extractive summary will try to select the most important steps from the protocol and remove any redundant information (if any). However, if we want to obtain a summary closer to an abstract, we will need to perform abstractive summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive summary\n",
    "For the abstractive summaries we will train a series of models with the protocol text without the abstract, and try to infer the abstract as an output. First, we will load this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['full_text_no_abstract_cleaned'].values\n",
    "y_true = df['abstract'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will also implement a series of functions which are common to every summarization model used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def trim_batch(input_ids, pad_token_id, attention_mask=None):\n",
    "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
    "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
    "    if attention_mask is None:\n",
    "        return input_ids[:, keep_column_mask]\n",
    "    else:\n",
    "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n",
    "\n",
    "def get_model_predictions(model, tokenizer, x):\n",
    "    return [_predict(model, tokenizer, doc) for doc in x]\n",
    "\n",
    "def _predict(model, tokenizer, doc):\n",
    "    batch = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(DEFAULT_DEVICE)\n",
    "    input_ids, attention_mask = trim_batch(**batch, pad_token_id=tokenizer.pad_token_id)\n",
    "    summaries = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_start_token_id=None\n",
    "    )\n",
    "    dec = tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return dec[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will be obtaining the summaries for 3 different models. The three of them are based on [BART](https://ai.facebook.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/), a denoising autoencoder for pretraining sequence-to-sequence models:\n",
    "* facebook/bart-large-cnn: Bart model trained on the [CNN/Dailymail summarization dataset](https://www.tensorflow.org/datasets/catalog/cnn_dailymail). This is a general purpose model used as a baseline to be compared with the following ones.\n",
    "* distillbart_cnn_protocols: This is a model where alternating layers of the bart-large-cnn model are copied and the others are finetuned on a subset of protocols.\n",
    "* distillbart_xsum_protocols: This model follows the same principles as the previous one, but the base model was trained on the [extreme summarization (XSum) dataset](https://www.tensorflow.org/datasets/catalog/xsum) instead of the CNN one.\n",
    "\n",
    "Due to the nature of the original datasets of the pretrained models, we expect the cnn variation of distillbart to produce longer summaries and the xsum variation to produce shorter ones. The model to use could be customized by the final user depending on the length of the summaries that is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dir = os.path.join(DATA_DIR, 'text_summarization_models')\n",
    "models_names = ['facebook/bart-large-cnn',\n",
    "                'distillbart_cnn_protocols',\n",
    "                'distillbart_xsum_protocols']\n",
    "\n",
    "model_results = {}\n",
    "for name in models_names:\n",
    "    model_path = name if 'distillbart' not in name \\\n",
    "                      else os.path.join(os.path.join(base_model_dir, name), 'best_tfmr')\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEFAULT_DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_results[name] = get_model_predictions(model, tokenizer, x)\n",
    "\n",
    "# add extractive results\n",
    "model_results['tf_extractive'] = extractive_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the models have been trained, we will select a sample protocol to see what the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "result_sample_idx = random.randint(0, 100)\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    print(model_name)\n",
    "    print(results[result_sample_idx])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now that we have obtained a predicted summary from every model, we will evaluate the results by comparing them to the abstracts of each protocol. To do so, we will be obtaining the rouge-1 and rouge-L score of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def _compute_mean(scores):\n",
    "    precision = [score.precision for score in scores]\n",
    "    recall = [score.recall for score in scores]\n",
    "    fmeasure = [score.fmeasure for score in scores]\n",
    "    return {\n",
    "        'precision': np.mean(precision),\n",
    "        'recall': np.mean(recall),\n",
    "        'fmeasure': np.mean(fmeasure)\n",
    "    }\n",
    "\n",
    "def compute_rouge_scores(y, results):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rougel_scores = []\n",
    "\n",
    "    for y_pred, y_true in zip(results, y):\n",
    "        rouge_score = scorer.score(y_pred, y_true)\n",
    "        rouge1_scores.append(rouge_score['rouge1'])\n",
    "        rougel_scores.append(rouge_score['rougeL'])\n",
    "\n",
    "    return {\n",
    "        'rouge1': _compute_mean(rouge1_scores),\n",
    "        'rougeL': _compute_mean(rougel_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for model_name, predictions in model_results.items():\n",
    "    model_scores[model_name] = compute_rouge_scores(predictions, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, scores in model_scores.items():\n",
    "    print(model_name)\n",
    "    print(scores)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the results, the _distillbart_cnn_protocols_ model obtains the best f1 scores, followed by the distillbart_xsum_protocols. Both the general purpose bart model and the TF extractive approach have worse scores.\n",
    "\n",
    "Regarding the differences between the xsum and cnn distilbart models, due to the differences in the predicted summary length explained before, it is expected that the xsum variation obtains a higher precision while the cnn one gets a higher recall. Depending on the needs of the final summary one model could be selected over the other. However, the cnn variation obtains the best f1 score overall and should be selected by default if in doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results\n",
    "Finally, we will be saving the results obtained in this notebook. We will first save the predictions made by each model, and later on their respective scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df[['pr_id', 'title', 'abstract']]\n",
    "for model_name, predictions in model_results.items():\n",
    "    results_df = results_df.assign(**{model_name: predictions})\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_OUTPUT_PATH = os.path.join(NOTEBOOK_8_RESULTS_DIR, 'predictions.csv')\n",
    "results_df.to_csv(PREDICTIONS_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame({\n",
    "    'model': list(model_scores.keys()),\n",
    "    'rouge1_prec': [v['rouge1']['precision'] for v in model_scores.values()],\n",
    "    'rouge1_recall': [v['rouge1']['recall'] for v in model_scores.values()],\n",
    "    'rouge1_fmeasure': [v['rouge1']['fmeasure'] for v in model_scores.values()],\n",
    "    'rougeL_prec': [v['rougeL']['precision'] for v in model_scores.values()],\n",
    "    'rougeL_recall': [v['rougeL']['recall'] for v in model_scores.values()],\n",
    "    'rougeL_fmeasure': [v['rougeL']['fmeasure'] for v in model_scores.values()],\n",
    "})\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES_OUTPUT_PATH = os.path.join(NOTEBOOK_8_RESULTS_DIR, 'scores.csv')\n",
    "scores_df.to_csv(SCORES_OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
