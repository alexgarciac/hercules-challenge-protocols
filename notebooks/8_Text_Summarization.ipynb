{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting logger\n"
     ]
    }
   ],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DF_FILE_PATH = os.path.join(NOTEBOOK_2_RESULTS_DIR, 'protocols_dataframe.pkl')\n",
    "\n",
    "df = pd.read_pickle(DF_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scratch Wound Healing Assay. Grow cells in DMEM supplemented with 10% FBS. Seed cells into 24-well tissue culture plate at a density that after 24 h of growth, they should reach ~70-80% confluence as a monolayer. Do not change the medium. Gently and slowly scratch the monolayer with a new 1 ml pipette tip across the center of the well. While scratching across the surface of the well, the long-axial of the tip should always be perpendicular to the bottom of the well. The resulting gap distance th'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols_no_abstract = df['full_text_no_abstract_cleaned'].values\n",
    "protocols_no_abstract[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "\n",
    "\n",
    "nlp = en_core_sci_lg.load()\n",
    "\n",
    "def extract_summary(text, limit):\n",
    "    keyword = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "        \n",
    "    sent_strength={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    sent_strength[sent]+=freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    sorted_x = sorted(sent_strength.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(len(sorted_x)):\n",
    "        summary.append(str(sorted_x[i][0]).capitalize())\n",
    "\n",
    "        counter += 1\n",
    "        if(counter >= limit):\n",
    "            break\n",
    "            \n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grow cells for additional 48 h (or the time required if different cells are used). Wash the cells twice with 1x pbs, then fix the cells with 3.7% paraformaldehye for 30 min. Seed cells into 24-well tissue culture plate at a density that after 24 h of growth, they should reach ~70-80% confluence as a monolayer. After scratching, gently wash the well twice with medium to remove the detached cells.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sentences = 4\n",
    "\n",
    "extractive_summaries = [extract_summary(t, num_sentences)\n",
    "                        for t in protocols_no_abstract]\n",
    "extractive_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['full_text_no_abstract_cleaned'].values\n",
    "y_true = df['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.6.0+cpu available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def trim_batch(input_ids, pad_token_id, attention_mask=None):\n",
    "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
    "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
    "    if attention_mask is None:\n",
    "        return input_ids[:, keep_column_mask]\n",
    "    else:\n",
    "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n",
    "\n",
    "def get_model_predictions(model, tokenizer, x):\n",
    "    return [_predict(model, tokenizer, doc) for doc in x]\n",
    "\n",
    "def _predict(model, tokenizer, doc):\n",
    "    batch = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(DEFAULT_DEVICE)\n",
    "    input_ids, attention_mask = trim_batch(**batch, pad_token_id=tokenizer.pad_token_id)\n",
    "    summaries = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_start_token_id=None\n",
    "    )\n",
    "    dec = tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return dec[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/config.json from cache at C:\\Users\\alex/.cache\\torch\\transformers\\5f0de1d2bbb8eb1a3b69656622293b3328b06b701663a9d4109359751cb4e739.5e72c6158467741b29afbcad014cd97414f17a191d39253eef90d7bfe969cc1f\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/facebook/bart-large-cnn/pytorch_model.bin from cache at C:\\Users\\alex/.cache\\torch\\transformers\\579dd21941940697e1fe35c8963e41bebe3260ff761dc99fe01f2d8f9a699996.73d71f0899e4bd27603a3503868c9f8cf938416df2de374c864a8c3af18f981d\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/config.json from cache at C:\\Users\\alex/.cache\\torch\\transformers\\5f0de1d2bbb8eb1a3b69656622293b3328b06b701663a9d4109359751cb4e739.5e72c6158467741b29afbcad014cd97414f17a191d39253eef90d7bfe969cc1f\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at C:\\Users\\alex/.cache\\torch\\transformers\\1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at C:\\Users\\alex/.cache\\torch\\transformers\\f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "base_model_dir = os.path.join(DATA_DIR, 'text_summarization_models')\n",
    "models_names = ['facebook/bart-large-cnn',\n",
    "                'distillbart_cnn_protocols',\n",
    "                'distillbart_xsum_protocols']\n",
    "\n",
    "model_results = {}\n",
    "for name in models_names:\n",
    "    model_path = name if 'distillbart' not in name \\\n",
    "                      else os.path.join(os.path.join(base_model_dir, name), 'best_tfmr')\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEFAULT_DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_results[name] = get_model_predictions(model, tokenizer, x)\n",
    "\n",
    "# add extractive results\n",
    "model_results['tfidf_extractive'] = extractive_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "result_sample_idx = random.randint(100)\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    print(model_name)\n",
    "    print(results[result_sample_idx])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def _compute_mean(scores):\n",
    "    precision = [score.precision for score in scores]\n",
    "    recall = [score.recall for score in scores]\n",
    "    fmeasure = [score.fmeasure for score in scores]\n",
    "    return {\n",
    "        'precision': np.mean(precision),\n",
    "        'recall': np.mean(recall),\n",
    "        'fmeasure': np.mean(fmeasure)\n",
    "    }\n",
    "\n",
    "def compute_rouge_scores(results, y):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rougel_scores = []\n",
    "\n",
    "    for y_pred, y_true in zip(results, y):\n",
    "        rouge_score = scorer.score(y_pred, y_true)\n",
    "        rouge1_scores.append(rouge_score['rouge1'])\n",
    "        rougel_scores.append(rouge_score['rougeL'])\n",
    "\n",
    "    return {\n",
    "        'rouge1': _compute_mean(rouge1_scores),\n",
    "        'rougeL': _compute_mean(rougel_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for model_name, predictions in model_results.items():\n",
    "    model_scores[model_name] = compute_rouge_scores(predictions[], y_true[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, scores in model_scores.items():\n",
    "    print(model_name)\n",
    "    print(scores)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df[['pr_id', 'title']]\n",
    "for model_name, predictions in model_results.items():\n",
    "    results_df = results_df.assign(**{model_name: predictions})\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_OUTPUT_PATH = os.path.join(NOTEBOOK_8_RESULTS_DIR, 'predictions.csv')\n",
    "results_df.to_csv(PREDICTIONS_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame({\n",
    "    'model': list(model_scores.keys()),\n",
    "    'rouge1_prec': [v['rouge1']['precision'] for v in model_scores.values()],\n",
    "    'rouge1_recall': [v['rouge1']['recall'] for v in model_scores.values()],\n",
    "    'rouge1_fmeasure': [v['rouge1']['fmeasure'] for v in model_scores.values()],\n",
    "    'rougeL_prec': [v['rougeL']['precision'] for v in model_scores.values()],\n",
    "    'rougeL_recall': [v['rougeL']['recall'] for v in model_scores.values()],\n",
    "    'rougeL_fmeasure': [v['rougeL']['fmeasure'] for v in model_scores.values()],\n",
    "})\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES_OUTPUT_PATH = os.path.join(NOTEBOOK_8_RESULTS_DIR, 'scores.csv')\n",
    "scores_df.to_csv(PREDICTIONS_OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
