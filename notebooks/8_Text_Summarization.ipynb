{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Text Summarization\n",
    "In this notebook we will be working on the creation of models to obtain a summary of the protocols from this track. We will be exploring two different types of summarization approaches:\n",
    "* Extractive summarization: This approach relies on detecting the important sections of the input text and using those sections to generate a summary which is a subset of the original text. No new text generated with this technique.\n",
    "* Abstractive summarization: This approach reproduces important information from the input text but generates new complementary text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "As always, we will be importing common constants of every notebook from the track, and then we will load the protocols dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting logger\n"
     ]
    }
   ],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DF_FILE_PATH = os.path.join(NOTEBOOK_2_RESULTS_DIR, 'protocols_dataframe.pkl')\n",
    "\n",
    "df = pd.read_pickle(DF_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains a column with the full text of the protocol without its abstract. Since the idea is to generate a summary of the protocol, and the abstract is in itself an abstractive summary, we will try to infer a good summary without relying on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scratch Wound Healing Assay. Grow cells in DMEM supplemented with 10% FBS. Seed cells into 24-well tissue culture plate at a density that after 24 h of growth, they should reach ~70-80% confluence as a monolayer. Do not change the medium. Gently and slowly scratch the monolayer with a new 1 ml pipette tip across the center of the well. While scratching across the surface of the well, the long-axial of the tip should always be perpendicular to the bottom of the well. The resulting gap distance th'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols_no_abstract = df['full_text_no_abstract_cleaned'].values\n",
    "protocols_no_abstract[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive summary\n",
    "For the extractive summary approach we will be relying on the term frequency of each term in a sentence to select the most relevant sentences that will then be added to the final summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "\n",
    "\n",
    "nlp = en_core_sci_lg.load()\n",
    "\n",
    "def extract_summary(text, max_sentences):\n",
    "    keyword = []\n",
    "    pos_tag = ['ADJ', 'NOUN', 'PROPN', 'VERB']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "        \n",
    "    sent_score={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_score.keys():\n",
    "                    sent_score[sent] += freq_word[word.text]\n",
    "                else:\n",
    "                    sent_score[sent] = freq_word[word.text]\n",
    "    \n",
    "    summary = []\n",
    "    sorted_x = sorted(sent_score.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    num_sentences = 0\n",
    "    for i in range(len(sorted_x)):\n",
    "        summary.append(str(sorted_x[i][0]).capitalize())\n",
    "        num_sentences += 1\n",
    "        if(counter >= max_sentences):\n",
    "            break\n",
    "            \n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grow cells for additional 48 h (or the time required if different cells are used). Wash the cells twice with 1x pbs, then fix the cells with 3.7% paraformaldehye for 30 min. Seed cells into 24-well tissue culture plate at a density that after 24 h of growth, they should reach ~70-80% confluence as a monolayer. After scratching, gently wash the well twice with medium to remove the detached cells.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sentences = 4\n",
    "\n",
    "extractive_summaries = [extract_summary(t, num_sentences)\n",
    "                        for t in protocols_no_abstract]\n",
    "extractive_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the protocols contain only the materials and procedure (list of steps of the protocol), an extractive summary will try to select the most important steps from the protocol and remove any redundant information (if any). However, if we want to obtain a summary closer to an abstract, we will need to perform abstractive summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive summary\n",
    "For the abstractive summaries we will train a series of models with the protocol text without the abstract, and try to infer the abstract as an output. First, we will load this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['full_text_no_abstract_cleaned'].values\n",
    "y_true = df['abstract'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will also implement a series of functions which are common to every summarization model used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.6.0+cpu available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def trim_batch(input_ids, pad_token_id, attention_mask=None):\n",
    "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
    "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
    "    if attention_mask is None:\n",
    "        return input_ids[:, keep_column_mask]\n",
    "    else:\n",
    "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n",
    "\n",
    "def get_model_predictions(model, tokenizer, x):\n",
    "    return [_predict(model, tokenizer, doc) for doc in x]\n",
    "\n",
    "def _predict(model, tokenizer, doc):\n",
    "    batch = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(DEFAULT_DEVICE)\n",
    "    input_ids, attention_mask = trim_batch(**batch, pad_token_id=tokenizer.pad_token_id)\n",
    "    summaries = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_start_token_id=None\n",
    "    )\n",
    "    dec = tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return dec[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will be obtaining the summaries for 3 different models. The three of them are based on [BART](https://ai.facebook.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/), a denoising autoencoder for pretraining sequence-to-sequence models:\n",
    "* facebook/bart-large-cnn: Bart model trained on the [CNN/Dailymail summarization dataset](https://www.tensorflow.org/datasets/catalog/cnn_dailymail). This is a general purpose model used as a baseline to be compared with the following ones.\n",
    "* distillbart_cnn_protocols: This is a model where alternating layers of the bart-large-cnn model are copied and the others are finetuned on a subset of protocols.\n",
    "* distillbart_xsum_protocols: This model follows the same principles as the previous one, but the base model was trained on the [extreme summarization (XSum) dataset](https://www.tensorflow.org/datasets/catalog/xsum) instead of the CNN one.\n",
    "\n",
    "Due to the nature of the original datasets of the pretrained models, we expect the cnn variation of distillbart to produce longer summaries and the xsum variation to produce shorter ones. The model to use could be customized by the final user depending on the length of the summaries that is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/config.json from cache at C:\\Users\\alex/.cache\\torch\\transformers\\5f0de1d2bbb8eb1a3b69656622293b3328b06b701663a9d4109359751cb4e739.5e72c6158467741b29afbcad014cd97414f17a191d39253eef90d7bfe969cc1f\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/facebook/bart-large-cnn/pytorch_model.bin from cache at C:\\Users\\alex/.cache\\torch\\transformers\\579dd21941940697e1fe35c8963e41bebe3260ff761dc99fe01f2d8f9a699996.73d71f0899e4bd27603a3503868c9f8cf938416df2de374c864a8c3af18f981d\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/config.json from cache at C:\\Users\\alex/.cache\\torch\\transformers\\5f0de1d2bbb8eb1a3b69656622293b3328b06b701663a9d4109359751cb4e739.5e72c6158467741b29afbcad014cd97414f17a191d39253eef90d7bfe969cc1f\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at C:\\Users\\alex/.cache\\torch\\transformers\\1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at C:\\Users\\alex/.cache\\torch\\transformers\\f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.configuration_utils:loading configuration file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"save_step\": 22,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\pytorch_model.bin\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "INFO:transformers.configuration_utils:loading configuration file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"save_step\": 22,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils_base:Model name 'E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils_base:Didn't find file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils_base:Didn't find file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\tokenizer.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\vocab.json\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\merges.txt\n",
      "INFO:transformers.tokenization_utils_base:loading file None\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\special_tokens_map.json\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_cnn_protocols\\best_tfmr\\tokenizer_config.json\n",
      "INFO:transformers.tokenization_utils_base:loading file None\n",
      "INFO:transformers.configuration_utils:loading configuration file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.5,\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"save_step\": 13,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {},\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\pytorch_model.bin\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "INFO:transformers.configuration_utils:loading configuration file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.5,\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"save_step\": 13,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {},\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils_base:Model name 'E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils_base:Didn't find file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils_base:Didn't find file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\tokenizer.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\vocab.json\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\merges.txt\n",
      "INFO:transformers.tokenization_utils_base:loading file None\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\special_tokens_map.json\n",
      "INFO:transformers.tokenization_utils_base:loading file E:\\hercules\\hercules-challenge-protocols\\data\\text_summarization_models\\distillbart_xsum_protocols\\best_tfmr\\tokenizer_config.json\n",
      "INFO:transformers.tokenization_utils_base:loading file None\n"
     ]
    }
   ],
   "source": [
    "base_model_dir = os.path.join(DATA_DIR, 'text_summarization_models')\n",
    "models_names = ['facebook/bart-large-cnn',\n",
    "                'distillbart_cnn_protocols',\n",
    "                'distillbart_xsum_protocols']\n",
    "\n",
    "model_results = {}\n",
    "for name in models_names:\n",
    "    model_path = name if 'distillbart' not in name \\\n",
    "                      else os.path.join(os.path.join(base_model_dir, name), 'best_tfmr')\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEFAULT_DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_results[name] = get_model_predictions(model, tokenizer, x)\n",
    "\n",
    "# add extractive results\n",
    "model_results['tf_extractive'] = extractive_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the models have been trained, we will select a sample protocol to see what the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/bart-large-cnn\n",
      "Gallyas Silver Impregnation of Myelinated Nerve Fibers. Carefully remove the brain or spinal cord and post-fix it in 4% PFA for tissue for one hour up to overnight at 4 °C in a 50 ml Falcon. Embed the brain in paraffin by using an automated embedding machine for best results.\n",
      "\n",
      "\n",
      "distillbart_cnn_protocols\n",
      " Myelinated Nerve Fibers are a multilayly, specialized, specialized membrane membrane membrane of the brain and spinal cord. Myelin is a critical component of the neurobiological process that is critical to the survival of myelin fibers in the brain. Here, we describe the protocol of paraffin embedding and microtome sectioning of brain tissue using Gallyas silver impregnation. This protocol provides a detailed understanding of the molecular molecular process of myelinated fibers.\n",
      "\n",
      "\n",
      "distillbart_xsum_protocols\n",
      " The Gallyas silver impregnation of myelinated nerve fibers (Gallyas Silver Impregnation) is described in a series of microtome sections of the brain. This protocol is based on a protocol based on paraffin embedding of brain and spinal cord sections.\n",
      "\n",
      "\n",
      "tfidf_extractive\n",
      "If no particular protocol is recommended, use the following protocol: reagent duration 50% ethanol 1 h 70% ethanol 2x 2 h 96% ethanol 2x 1 h 100% ethanol 2x 1 h 2-propanol 1 h xylene 2x 2 h paraffin 2x 2 h cut 5 µm sections using a microtome and mount sections on microscope glass slides. Gallyas silver impregnation (overview of protocol see figure 1) de-paraffinize sections by incubating for 10 min each in xylene, again xylene and 2-propanol/xylene (1:1). To suppress staining of non-myelin tissue, incubate sections in pyridine/acetic anhydride (2:1, e.g., 200 ml of pyridine and 100 ml of acetic anhydride) for 30 min at room temperature (rt). However, even largely unmyelinated cns regions including the cortex comprise individual myelinated axons that appear as fine fibers if visualized by specific staining.ferenc gallyas described in 1979 a method to specifically visualize myelin in the brain (gallyas, 1979).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "result_sample_idx = random.randint(0, 100)\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    print(model_name)\n",
    "    print(results[result_sample_idx])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now that we have obtained a predicted summary from every model, we will evaluate the results by comparing them to the abstracts of each protocol. To do so, we will be obtaining the rouge-1 and rouge-L score of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def _compute_mean(scores):\n",
    "    precision = [score.precision for score in scores]\n",
    "    recall = [score.recall for score in scores]\n",
    "    fmeasure = [score.fmeasure for score in scores]\n",
    "    return {\n",
    "        'precision': np.mean(precision),\n",
    "        'recall': np.mean(recall),\n",
    "        'fmeasure': np.mean(fmeasure)\n",
    "    }\n",
    "\n",
    "def compute_rouge_scores(y, results):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rougel_scores = []\n",
    "\n",
    "    for y_pred, y_true in zip(results, y):\n",
    "        rouge_score = scorer.score(y_pred, y_true)\n",
    "        rouge1_scores.append(rouge_score['rouge1'])\n",
    "        rougel_scores.append(rouge_score['rougeL'])\n",
    "\n",
    "    return {\n",
    "        'rouge1': _compute_mean(rouge1_scores),\n",
    "        'rougeL': _compute_mean(rougel_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for model_name, predictions in model_results.items():\n",
    "    model_scores[model_name] = compute_rouge_scores(predictions, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/bart-large-cnn\n",
      "{'rouge1': {'precision': 0.35190216760012255, 'recall': 0.1970650603655765, 'fmeasure': 0.24239708155727885}, 'rougeL': {'precision': 0.2222215001177071, 'recall': 0.12408936764926332, 'fmeasure': 0.15280770125052318}}\n",
      "\n",
      "\n",
      "distillbart_cnn_protocols\n",
      "{'rouge1': {'precision': 0.474512108884034, 'recall': 0.3355643603445984, 'fmeasure': 0.3774299921065759}, 'rougeL': {'precision': 0.2692036410087131, 'recall': 0.19382174285485979, 'fmeasure': 0.21693775860376777}}\n",
      "\n",
      "\n",
      "distillbart_xsum_protocols\n",
      "{'rouge1': {'precision': 0.5958868276552898, 'recall': 0.2338036468809735, 'fmeasure': 0.32253741512607187}, 'rougeL': {'precision': 0.38956795161294017, 'recall': 0.15703934413935447, 'fmeasure': 0.2147638629198031}}\n",
      "\n",
      "\n",
      "tfidf_extractive\n",
      "{'rouge1': {'precision': 0.2284128492474408, 'recall': 0.3318414622415259, 'fmeasure': 0.24812294545758565}, 'rougeL': {'precision': 0.13074264248047132, 'recall': 0.18895182529229163, 'fmeasure': 0.1404100208081965}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, scores in model_scores.items():\n",
    "    print(model_name)\n",
    "    print(scores)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the results, the _distillbart_cnn_protocols_ model obtains the best f1 scores, followed by the distillbart_xsum_protocols. Both the general purpose bart model and the TF extractive approach have worse scores.\n",
    "\n",
    "Regarding the differences between the xsum and cnn distilbart models, due to the differences in the predicted summary length explained before, it is expected that the xsum variation obtains a higher precision while the cnn one gets a higher recall. Depending on the needs of the final summary one model could be selected over the other. However, the cnn variation obtains the best f1 score overall and should be selected by default if in doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results\n",
    "Finally, we will be saving the results obtained in this notebook. We will first save the predictions made by each model, and later on their respective scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>facebook/bart-large-cnn</th>\n",
       "      <th>distillbart_cnn_protocols</th>\n",
       "      <th>distillbart_xsum_protocols</th>\n",
       "      <th>tfidf_extractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e100</td>\n",
       "      <td>Scratch Wound Healing Assay</td>\n",
       "      <td>The scratch wound healing assay has been widel...</td>\n",
       "      <td>Scratch Wound Healing Assay. Grow cells in DME...</td>\n",
       "      <td>The scratch wound healing assay is a useful t...</td>\n",
       "      <td>Scratch wound healing assay (Scratch Wound He...</td>\n",
       "      <td>Grow cells for additional 48 h (or the time re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1029</td>\n",
       "      <td>ADCC Assay Protocol</td>\n",
       "      <td>Antibody-dependent cell-mediated cytotoxicity ...</td>\n",
       "      <td>ADCC Assay Protocol. Infect Raji cells at a MO...</td>\n",
       "      <td>Antibody-dependent cell-mediated cytotoxicity...</td>\n",
       "      <td>Antibody-dependent cell-mediated cytotoxicity...</td>\n",
       "      <td>1 2 3 4 5 6 7 unstained target cells unstained...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1072</td>\n",
       "      <td>Catalase Activity Assay in Candida glabrata</td>\n",
       "      <td>Commensal and pathogenic fungi are exposed to ...</td>\n",
       "      <td>Catalase Activity Assay in Candida glabrata. Y...</td>\n",
       "      <td>This protocol describes the preparation of to...</td>\n",
       "      <td>Candida glabrata is a type of yeast that can ...</td>\n",
       "      <td>Preparation of total soluble extracts yeast st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1077</td>\n",
       "      <td>RNA Isolation and Northern Blot Analysis</td>\n",
       "      <td>The northern blot is a technique used in molec...</td>\n",
       "      <td>Cells were infected with either 2 PFU of live ...</td>\n",
       "      <td>The northern blot is a technique used in vitr...</td>\n",
       "      <td>Northern blot analysis is based on an RNA iso...</td>\n",
       "      <td>Place the sample in a thermal block cycler and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e1090</td>\n",
       "      <td>Flow Cytometric Analysis of Autophagic Activit...</td>\n",
       "      <td>Flow cytometry allows very sensitive and relia...</td>\n",
       "      <td>Flow Cytometric Analysis of Autophagic Activit...</td>\n",
       "      <td>Flow Cytometric analysis of autophagic flux i...</td>\n",
       "      <td>Autophagic flux in primary cells is determine...</td>\n",
       "      <td>Positive control [rapamycin (1-5 µmol/l), pp24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pr_id                                              title  \\\n",
       "0   e100                        Scratch Wound Healing Assay   \n",
       "1  e1029                                ADCC Assay Protocol   \n",
       "2  e1072        Catalase Activity Assay in Candida glabrata   \n",
       "3  e1077           RNA Isolation and Northern Blot Analysis   \n",
       "4  e1090  Flow Cytometric Analysis of Autophagic Activit...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The scratch wound healing assay has been widel...   \n",
       "1  Antibody-dependent cell-mediated cytotoxicity ...   \n",
       "2  Commensal and pathogenic fungi are exposed to ...   \n",
       "3  The northern blot is a technique used in molec...   \n",
       "4  Flow cytometry allows very sensitive and relia...   \n",
       "\n",
       "                             facebook/bart-large-cnn  \\\n",
       "0  Scratch Wound Healing Assay. Grow cells in DME...   \n",
       "1  ADCC Assay Protocol. Infect Raji cells at a MO...   \n",
       "2  Catalase Activity Assay in Candida glabrata. Y...   \n",
       "3  Cells were infected with either 2 PFU of live ...   \n",
       "4  Flow Cytometric Analysis of Autophagic Activit...   \n",
       "\n",
       "                           distillbart_cnn_protocols  \\\n",
       "0   The scratch wound healing assay is a useful t...   \n",
       "1   Antibody-dependent cell-mediated cytotoxicity...   \n",
       "2   This protocol describes the preparation of to...   \n",
       "3   The northern blot is a technique used in vitr...   \n",
       "4   Flow Cytometric analysis of autophagic flux i...   \n",
       "\n",
       "                          distillbart_xsum_protocols  \\\n",
       "0   Scratch wound healing assay (Scratch Wound He...   \n",
       "1   Antibody-dependent cell-mediated cytotoxicity...   \n",
       "2   Candida glabrata is a type of yeast that can ...   \n",
       "3   Northern blot analysis is based on an RNA iso...   \n",
       "4   Autophagic flux in primary cells is determine...   \n",
       "\n",
       "                                    tfidf_extractive  \n",
       "0  Grow cells for additional 48 h (or the time re...  \n",
       "1  1 2 3 4 5 6 7 unstained target cells unstained...  \n",
       "2  Preparation of total soluble extracts yeast st...  \n",
       "3  Place the sample in a thermal block cycler and...  \n",
       "4  Positive control [rapamycin (1-5 µmol/l), pp24...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = df[['pr_id', 'title', 'abstract']]\n",
    "for model_name, predictions in model_results.items():\n",
    "    results_df = results_df.assign(**{model_name: predictions})\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_OUTPUT_PATH = os.path.join(NOTEBOOK_8_RESULTS_DIR, 'predictions.csv')\n",
    "results_df.to_csv(PREDICTIONS_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1_prec</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <th>rougeL_prec</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>0.363438</td>\n",
       "      <td>0.178974</td>\n",
       "      <td>0.218508</td>\n",
       "      <td>0.233888</td>\n",
       "      <td>0.115813</td>\n",
       "      <td>0.141020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distillbart_cnn_protocols</td>\n",
       "      <td>0.517355</td>\n",
       "      <td>0.320229</td>\n",
       "      <td>0.363781</td>\n",
       "      <td>0.322455</td>\n",
       "      <td>0.207531</td>\n",
       "      <td>0.230837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distillbart_xsum_protocols</td>\n",
       "      <td>0.616950</td>\n",
       "      <td>0.206607</td>\n",
       "      <td>0.288678</td>\n",
       "      <td>0.401467</td>\n",
       "      <td>0.133659</td>\n",
       "      <td>0.186940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_extractive</td>\n",
       "      <td>0.218440</td>\n",
       "      <td>0.282621</td>\n",
       "      <td>0.229900</td>\n",
       "      <td>0.121314</td>\n",
       "      <td>0.159748</td>\n",
       "      <td>0.128178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  rouge1_prec  rouge1_recall  rouge1_fmeasure  \\\n",
       "0     facebook/bart-large-cnn     0.363438       0.178974         0.218508   \n",
       "1   distillbart_cnn_protocols     0.517355       0.320229         0.363781   \n",
       "2  distillbart_xsum_protocols     0.616950       0.206607         0.288678   \n",
       "3            tfidf_extractive     0.218440       0.282621         0.229900   \n",
       "\n",
       "   rougeL_prec  rougeL_recall  rougeL_fmeasure  \n",
       "0     0.233888       0.115813         0.141020  \n",
       "1     0.322455       0.207531         0.230837  \n",
       "2     0.401467       0.133659         0.186940  \n",
       "3     0.121314       0.159748         0.128178  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame({\n",
    "    'model': list(model_scores.keys()),\n",
    "    'rouge1_prec': [v['rouge1']['precision'] for v in model_scores.values()],\n",
    "    'rouge1_recall': [v['rouge1']['recall'] for v in model_scores.values()],\n",
    "    'rouge1_fmeasure': [v['rouge1']['fmeasure'] for v in model_scores.values()],\n",
    "    'rougeL_prec': [v['rougeL']['precision'] for v in model_scores.values()],\n",
    "    'rougeL_recall': [v['rougeL']['recall'] for v in model_scores.values()],\n",
    "    'rougeL_fmeasure': [v['rougeL']['fmeasure'] for v in model_scores.values()],\n",
    "})\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES_OUTPUT_PATH = os.path.join(NOTEBOOK_8_RESULTS_DIR, 'scores.csv')\n",
    "scores_df.to_csv(SCORES_OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
